{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataset import load_training_data\n",
    "import pycrfsuite\n",
    "import json\n",
    "from src.pipelines import SentenceChunker\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"offer_len\": 44,\n",
      "    \"token\": \"\\u00a1\",\n",
      "    \"loc\": 0,\n",
      "    \"pos\": \"faa\",\n",
      "    \"pos_left\": \"<p>\",\n",
      "    \"pos_right\": \"np00000\",\n",
      "    \"token_len\": 1,\n",
      "    \"all_upper\": false,\n",
      "    \"n_tokens\": 11,\n",
      "    \"real_label\": \"n\",\n",
      "    \"sentence_id\": 0\n",
      "  }\n",
      "]\n",
      "\n",
      "Training docs: 170\n",
      "Testing docs: 57\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "training_set = load_training_data()\n",
    "training_set['real_label'] = training_set['real_label'].replace('f', 'n')\n",
    "\n",
    "\n",
    "\n",
    "documents = []\n",
    "current_doc = []\n",
    "prev = -1\n",
    "for i,word in training_set.iterrows():\n",
    "    if i != prev:\n",
    "        if current_doc:\n",
    "            documents.append(current_doc)\n",
    "        current_doc = []\n",
    "    word_dictionary = word.to_dict()    \n",
    "    word_dictionary['sentence_id'] = i\n",
    "    current_doc.append(word_dictionary)\n",
    "    prev = i\n",
    "\n",
    "if current_doc:\n",
    "    documents.append(current_doc)\n",
    "\n",
    "print(json.dumps( documents[0][:1], indent=2 ))\n",
    "\n",
    "train_docs, test_docs = train_test_split(documents)\n",
    "print()\n",
    "print(f'Training docs: {len(train_docs)}')\n",
    "print(f'Testing docs: {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def extract_labels(doc):\n",
    "    return [doc[i]['real_label'] for i in range(len(doc))]\n",
    "\n",
    "def extract_tokens(doc):\n",
    "    return [doc[i]['token'] for i in range(len(doc))]\n",
    "\n",
    "def is_numeric(token):\n",
    "    try:\n",
    "        float(token.replace(\",\", \"\"))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def word2features(doc, i):\n",
    "    word = doc[i]['token']\n",
    "    postag = doc[i]['pos']\n",
    "\n",
    "    # Common features for all words. You may add more features here based on your custom use case\n",
    "    features = [\n",
    "            'bias',\n",
    "#            'word.lower=' + word.lower(),\n",
    "#            'word[-3:]=' + word[-3:],\n",
    "#            'word[-2:]=' + word[-2:],\n",
    "#            'word.isupper=%s' % word.isupper(),\n",
    "#            'word.istitle=%s' % word.istitle(),\n",
    "            'word.isdigit=%s' % is_numeric(word),\n",
    "#            'word.location=%s' % doc[i]['loc'],\n",
    "            'postag=' + postag\n",
    "        ]\n",
    "\n",
    "    # Features for words that are not at the beginning of a document\n",
    "    if i > 0:\n",
    "            word1 = doc[i-1]['token']\n",
    "            postag1 = doc[i-1]['pos']\n",
    "            features.extend([\n",
    "#                '-1:word.lower=' + word1.lower(),\n",
    "#                '-1:word.istitle=%s' % word1.istitle(),\n",
    "#                '-1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isdigit=%s' % is_numeric(word1),\n",
    "                '-1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "            word1 = doc[i+1]['token']\n",
    "            postag1 = doc[i+1]['pos']\n",
    "            features.extend([\n",
    "#                '+1:word.lower=' + word1.lower(),\n",
    "#                '+1:word.istitle=%s' % word1.istitle(),\n",
    "#                '+1:word.isupper=%s' % word1.isupper(),\n",
    "                '+1:word.isdigit=%s' % is_numeric(word1),\n",
    "                '+1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train = [extract_labels(s) for s in train_docs]\n",
    "X_train = [extract_features(s) for s in train_docs]\n",
    "\n",
    "y_test = [extract_labels(s) for s in test_docs]\n",
    "X_test = [extract_features(s) for s in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incorrect = 0\n",
    "for i in range(len(test_docs)):\n",
    "    example_sent = documents[i]\n",
    "    predicted = tagger.tag(extract_features(example_sent))\n",
    "    correct = extract_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = extract_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0]['sentence_id'], ' '.join(tokens))\n",
    "        \n",
    "        print('P:   ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('C:   ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = 0\n",
    "for i in range(len(documents)):\n",
    "    example_sent = documents[i]\n",
    "    predicted = tagger.tag(extract_features(example_sent))\n",
    "    correct = extract_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = extract_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0]['sentence_id'], ' '.join(tokens))\n",
    "        \n",
    "        print('P:   ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('C:   ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
