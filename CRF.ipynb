{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import load_training_data\n",
    "from features import index_emoji_tokenize, is_numeric, is_punctuation, row_to_tokenfeatures, TokenFeatures\n",
    "import pycrfsuite\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import string\n",
    "from pipelines import SentenceChunker\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform new sentences to TokenFeatures\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "tagger = StanfordPOSTagger('tagger/spanish.tagger',\n",
    "                           'tagger/stanford-postagger.jar')\n",
    "\n",
    "\n",
    "def process_sentence(sentence, sentence_id=None):\n",
    "    tokens = list(index_emoji_tokenize(sentence, True))\n",
    "    only_tokens = [l[0] for l in tokens]\n",
    "    positions = [l[1] for l in tokens]\n",
    "    tagged = tagger.tag(only_tokens)\n",
    "    tags = [l[1] for l in tagged]\n",
    "    lengths = [len(l) for l in only_tokens]\n",
    "    offer_len = [len(sentence) for l in only_tokens]\n",
    "    n_tokens = [len(only_tokens) for l in only_tokens]\n",
    "    augmented = ['<p>'] + tags + ['</p>']\n",
    "    uppercase = [all([l.isupper() for l in token]) for token in only_tokens]\n",
    "    numeric = [is_numeric(l) for l in only_tokens]\n",
    "    puntctuations = [is_punctuation(l) for l in only_tokens]\n",
    "    labels = [None for l in only_tokens]\n",
    "    sentence_ids = [sentence_id] * len(only_tokens)\n",
    "    return [TokenFeatures(*t) for t in zip(sentence_ids, offer_len, only_tokens, positions, tags, augmented[:len(only_tokens)], augmented[2:], \n",
    "                                           lengths, uppercase, n_tokens, numeric, puntctuations, labels)]\n",
    "\n",
    "process_sentence('\u00a1CUN a Madrid $200!', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform previous dataset to TokenFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenFeatures(sentence_id=0, offer_length=44, token='\u00a1', position=0, POS='faa', left_POS='<p>', right_POS='np00000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='CUN', position=1, POS='np00000', left_POS='faa', right_POS='sp000', token_length=3, uppercase=True, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='o'), TokenFeatures(sentence_id=0, offer_length=44, token='a', position=5, POS='sp000', left_POS='np00000', right_POS='np00000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='s'), TokenFeatures(sentence_id=0, offer_length=44, token='\u00c1msterdam', position=7, POS='np00000', left_POS='sp000', right_POS='zm', token_length=9, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='d'), TokenFeatures(sentence_id=0, offer_length=44, token='$', position=17, POS='zm', left_POS='np00000', right_POS='dn0000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=True, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='8,960', position=18, POS='dn0000', left_POS='zm', right_POS='fat', token_length=5, uppercase=False, tokens_in_sentence=11, is_numeric=True, is_punctuation=False, label='p'), TokenFeatures(sentence_id=0, offer_length=44, token='!', position=23, POS='fat', left_POS='dn0000', right_POS='sp000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=True, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='Sin', position=25, POS='sp000', left_POS='fat', right_POS='nc0s000', token_length=3, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='escala', position=29, POS='nc0s000', left_POS='sp000', right_POS='sp000', token_length=6, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='en', position=36, POS='sp000', left_POS='nc0s000', right_POS='np00000', token_length=2, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='EE.UU', position=39, POS='np00000', left_POS='sp000', right_POS='</p>', token_length=5, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n')]\n",
      "\n",
      "Training docs: 178\n",
      "Testing docs: 60\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "training_set = load_training_data()\n",
    "training_set['real_label'] = training_set['real_label'].replace('f', 'n')\n",
    "\n",
    "documents = []\n",
    "current_doc = []\n",
    "prev = -1\n",
    "for i,word in training_set.iterrows():\n",
    "    if i != prev:\n",
    "        if current_doc:\n",
    "            documents.append(current_doc)\n",
    "        current_doc = []\n",
    "    current_doc.append(row_to_tokenfeatures(word))\n",
    "    prev = i\n",
    "\n",
    "if current_doc:\n",
    "    documents.append(current_doc)\n",
    "\n",
    "print(documents[0])\n",
    "\n",
    "train_docs, test_docs = train_test_split(documents)\n",
    "print()\n",
    "print(f'Training docs: {len(train_docs)}')\n",
    "print(f'Testing docs: {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def extract_labels(doc):\n",
    "    return [doc[i].label for i in range(len(doc))]\n",
    "\n",
    "def extract_tokens(doc):\n",
    "    return [doc[i].token for i in range(len(doc))]\n",
    "    \n",
    "\n",
    "def word2features(doc, i):\n",
    "    tkn = doc[i]\n",
    "    word = doc[i].token\n",
    "    postag = doc[i].POS\n",
    "\n",
    "    # Common features for all words. You may add more features here based on your custom use case\n",
    "    features = [\n",
    "            'bias',\n",
    "            'word.lower=' + word.lower(),\n",
    "#            'word[-3:]=' + word[-3:],\n",
    "#            'word[-2:]=' + word[-2:],\n",
    "#            'word.isupper=%s' % word.isupper(),\n",
    "            'word.istitle=%s' % word.istitle(),\n",
    "            'word.isdigit=%s' % tkn.is_numeric,\n",
    "#            'word.ispunct=%s' % tkn.is_punctuation,\n",
    "#            'word.location=%s' % doc[i]['loc'],\n",
    "            'postag=' + postag\n",
    "        ]\n",
    "\n",
    "    # Features for words that are not at the beginning of a document\n",
    "    if i > 0:\n",
    "            tkn1 = doc[i-1]\n",
    "            word1 = doc[i-1].token\n",
    "            postag1 = doc[i-1].POS\n",
    "            features.extend([\n",
    "                '-1:word.lower=' + word1.lower(),\n",
    "                '-1:word.istitle=%s' % word1.istitle(),\n",
    "#                '-1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isdigit=%s' % tkn1.is_numeric,\n",
    "                '-1:word.ispunct=%s' % tkn1.is_punctuation,\n",
    "                '-1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "            tkn1 = doc[i+1]\n",
    "            word1 = doc[i+1].token\n",
    "            postag1 = doc[i+1].POS\n",
    "            features.extend([\n",
    "                '+1:word.lower=' + word1.lower(),\n",
    "                '+1:word.istitle=%s' % word1.istitle(),\n",
    "#                '+1:word.isupper=%s' % word1.isupper(),\n",
    "                '+1:word.isdigit=%s' % tkn1.is_numeric,\n",
    "                '+1:word.ispunct=%s' % tkn1.is_punctuation,\n",
    "                '+1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train = [extract_labels(s) for s in train_docs]\n",
    "X_train = [extract_features(s) for s in train_docs]\n",
    "\n",
    "y_test = [extract_labels(s) for s in test_docs]\n",
    "X_test = [extract_features(s) for s in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sizes\n",
    "for features, labels in zip(y_test, X_test):\n",
    "    assert len(features) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_tagger = pycrfsuite.Tagger()\n",
    "crf_tagger.open('model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40 \u00a1 CDMX a Europa en Semana Santa $ 14,984 ! ( Par\u00eds + Ibiza + Venecia )\n",
      "P:   n o    s d      d  d      d     n p      n n n     n n     n n       n \n",
      "C:   n o    s d      n  n      n     n p      n n n     n n     n n       n \n",
      "\n",
      "\n",
      "Incorrectly predicted: 1 out of 60\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "incorrect = 0\n",
    "for i in range(len(test_docs)):\n",
    "    example_sent = documents[i]\n",
    "    predicted = crf_tagger.tag(extract_features(example_sent))\n",
    "    correct = extract_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = extract_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "        \n",
    "        print('P:   ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('C:   ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40 \u00a1 CDMX a Europa en Semana Santa $ 14,984 ! ( Par\u00eds + Ibiza + Venecia )\n",
      "  P: n o    s d      d  d      d     n p      n n n     n n     n n       n \n",
      "  C: n o    s d      n  n      n     n p      n n n     n n     n n       n \n",
      "\n",
      "\n",
      "  60 \u00a1 CDMX a Noruega $ 10,061 ! ( Y agrega 9 noches de hotel por $ 7,890 !\n",
      "  P: n o    s d       n p      n n n n      n n      n  n     n   n p     n \n",
      "  C: n o    s d       n p      n n n n      n n      n  n     n   n n     n \n",
      "\n",
      "\n",
      " 171 \u00a1 CUN a King \u2019 s Landing ( Croacia ) + \u00c1msterdam $ 12,549 ! Sin escala EE.UU\n",
      "  P: n o   s d    n n n       n n       n n n         n p      n n   n      n     \n",
      "  C: n o   s d    d d d       d d       d d d         n p      n n   n      n     \n",
      "\n",
      "\n",
      " 202 \u00a1 CUN a Washington D . C . $ 3,196 ! Directos ( Y por $ 3,027 adicionales agrega 3 noches de hotel )\n",
      "  P: n o   s d          d d d n n p     n n        n n n   n n     n           n      n n      n  n     n \n",
      "  C: n o   s d          d d d d n p     n n        n n n   n n     n           n      n n      n  n     n \n",
      "\n",
      "\n",
      " 210 Agotados : \u00a1 CDMX a Europa Navidad y A\u00f1o Nuevo $ 16,424 ! ( Madrid + Par\u00eds + Venecia + Berl\u00edn )\n",
      "  P: n        n n o    s d      d       n n   n     n p      n n n      n n     n n       n n      n \n",
      "  C: n        n n o    s d      n       n n   n     n p      n n n      n n     n n       n n      n \n",
      "\n",
      "\n",
      " 401 \u00a1 CUN a Bogot\u00e1 , Colombia \u2013 $ 3,860 ! Opci\u00f3n de hotel 5 d\u00edas y 4 noches por $ 858 p / persona con desayunos ( hab doble ) . Hostal con desayunos por $ 634\n",
      "  P: n o   s d      d d        n n p     n n      n  n     n n    n n n      n   n n   n n n       n   n         n n   n     n n n      n   n         n   n p   \n",
      "  C: n o   s d      d d        n n p     n n      n  n     n n    n n n      n   n n   n n n       n   n         n n   n     n n n      n   n         n   n n   \n",
      "\n",
      "\n",
      " 727 \u00a1 Vuelos + Hotel ! Silao y Puebla a Playa del Carmen \u2013 $ 1,557 . CDMX desde $ 2,451 con desayunos\n",
      "  P: n o      o o     o o     o o      s d     d   d      n n n     n n    n     n n     n   n         \n",
      "  C: n n      n n     n o     o o      s d     d   d      n n p     n n    n     n n     n   n         \n",
      "\n",
      "\n",
      "1310 Vuelo + Hotel . Desde todo M\u00e9xico . CDMX y 23 ciudades m\u00e1s a Santo Domingo , Rep\u00fablica Dominicana \u2013 $ 9,171\n",
      "  P: n     n n     n n     n    o      o o    o o  o        o   s d     d       d d         d          n n p     \n",
      "  C: n     n n     n n     o    o      o o    o o  o        o   s d     d       d d         d          n n p     \n",
      "\n",
      "\n",
      "2125 \u00a1 \u00daltimo minuto ! Oportunidades para semana santa : Cuba + Panam\u00e1 y Chicago\n",
      "  P: n n      n      n n             n    n      n     n n    n n      n n       \n",
      "  C: n n      n      n n             n    n      n     n d    d d      d d       \n",
      "\n",
      "\n",
      "Incorrectly predicted: 9 out of 238\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "incorrect = 0\n",
    "for i in range(len(documents)):\n",
    "    example_sent = documents[i]\n",
    "    features = extract_features(example_sent)\n",
    "    predicted = crf_tagger.tag(features)\n",
    "    correct = extract_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = extract_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "        \n",
    "        print('  P: ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('  C: ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 \u00a1 CDMX a Bogot\u00e1 $ 3,467 ! Directos ( Agrega 2 noches de hotel por $ 419 )\n",
      "  P: n o    s d      n p     n n        n n      n n      n  n     n   n n   n \n",
      "\n",
      "\n",
      "   1 \u00a1 CUN a Israel $ 14,574 ! Sin escala EE.UU ( y desde CDMX $ 15,146 )\n",
      "  P: n o   s d      n p      n n   n      n     n n n     n    n n      n \n",
      "\n",
      "\n",
      "   2 \u00a1 CDMX , GDL , VER , MTY , CUN , Silao y TIJ a Lima , Per\u00fa \u2013 $ 6,529 !\n",
      "  P: n o    o o   o o   o o   o o   o o     o o   s d    d d    n n p     n \n",
      "\n",
      "\n",
      "   3 \u00a1 CDMX a Noruega $ 11,863 ! Temporada de Auroras\n",
      "  P: n o    s d       n p      n n         n  n       \n",
      "\n",
      "\n",
      "   4 \u00a1 Tijuana a China + Corea $ 17,522 ! Sin escala EE.UU ( Sem . Santa )\n",
      "  P: n o       s d     d d     n p      n n   n      n     n n   n n     n \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "manual_examples = ['\u00a1CDMX a Bogot\u00e1 $3,467! Directos (Agrega 2 noches de hotel por $419)',\n",
    "                   '\u00a1CUN a Israel $14,574! Sin escala EE.UU (y desde CDMX $15,146)',\n",
    "                   '\u00a1CDMX, GDL, VER, MTY, CUN, Silao y TIJ a Lima, Per\u00fa \u2013 $6,529! ',\n",
    "                   '\u00a1CDMX a Noruega $11,863! Temporada de Auroras',\n",
    "                   '\u00a1Tijuana a China + Corea $17,522! Sin escala EE.UU (Sem. Santa)']\n",
    "documents_as_token_features = [process_sentence(example, i) for i, example in enumerate(manual_examples)]\n",
    "documents_as_tagger_features = [extract_features(doc) for doc in documents_as_token_features]\n",
    "\n",
    "for i in range(len(documents_as_token_features)):\n",
    "    example_sent = documents_as_token_features[i]\n",
    "    features = extract_features(example_sent)\n",
    "    predicted = crf_tagger.tag(features)\n",
    "    correct = extract_labels(example_sent)\n",
    "    tokens = extract_tokens(example_sent)\n",
    "    lengths = [len(t) for t in tokens]\n",
    "    \n",
    "    print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "\n",
    "    print('  P: ', end='')\n",
    "    for i, token in enumerate(predicted):\n",
    "        print(token + ( \" \" * lengths[i]), end='')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_flat = [item for sublist in y_test for item in sublist]\n",
    "y_pred_flat = []\n",
    "\n",
    "\n",
    "from m16_mlutils.datatools.evaluation import eval_summary\n",
    "\n",
    "for doc in X_test:\n",
    "    predicted = crf_tagger.tag(doc)\n",
    "    y_pred_flat.extend(predicted)\n",
    "\n",
    "print(len(y_pred_flat), len(y_test_flat))\n",
    "metrics, summary, cm = eval_summary(y_test_flat, y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy     0.994037\n",
      "precision    0.992306\n",
      "recall       0.988180\n",
      "f1           0.990022\n",
      "dtype: float64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           d       1.00      0.94      0.97        88\n",
      "           n       0.99      1.00      1.00       877\n",
      "           o       1.00      1.00      1.00        91\n",
      "           p       0.97      1.00      0.98        59\n",
      "           s       1.00      1.00      1.00        59\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1174\n",
      "   macro avg       0.99      0.99      0.99      1174\n",
      "weighted avg       0.99      0.99      0.99      1174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "print(metrics, end='\\n\\n')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
