{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataset import load_training_data\n",
    "from extractor import get_features, get_tokens, get_labels\n",
    "from features import SentenceProcessor, row_to_tokenfeatures\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transform new sentences to TokenFeatures\n",
    "processor = SentenceProcessor('tagger/spanish.tagger',\n",
    "                              'tagger/stanford-postagger.jar')\n",
    "processor.process('\u00a1CUN a Madrid $200!', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform previous dataset to TokenFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenFeatures(sentence_id=0, offer_length=44, token='\u00a1', position=0, POS='faa', left_POS='<p>', right_POS='np00000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='CUN', position=1, POS='np00000', left_POS='faa', right_POS='sp000', token_length=3, uppercase=True, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='o'), TokenFeatures(sentence_id=0, offer_length=44, token='a', position=5, POS='sp000', left_POS='np00000', right_POS='np00000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='s'), TokenFeatures(sentence_id=0, offer_length=44, token='\u00c1msterdam', position=7, POS='np00000', left_POS='sp000', right_POS='zm', token_length=9, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='d'), TokenFeatures(sentence_id=0, offer_length=44, token='$', position=17, POS='zm', left_POS='np00000', right_POS='dn0000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=True, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='8,960', position=18, POS='dn0000', left_POS='zm', right_POS='fat', token_length=5, uppercase=False, tokens_in_sentence=11, is_numeric=True, is_punctuation=False, label='p'), TokenFeatures(sentence_id=0, offer_length=44, token='!', position=23, POS='fat', left_POS='dn0000', right_POS='sp000', token_length=1, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=True, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='Sin', position=25, POS='sp000', left_POS='fat', right_POS='nc0s000', token_length=3, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='escala', position=29, POS='nc0s000', left_POS='sp000', right_POS='sp000', token_length=6, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='en', position=36, POS='sp000', left_POS='nc0s000', right_POS='np00000', token_length=2, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n'), TokenFeatures(sentence_id=0, offer_length=44, token='EE.UU', position=39, POS='np00000', left_POS='sp000', right_POS='</p>', token_length=5, uppercase=False, tokens_in_sentence=11, is_numeric=False, is_punctuation=False, label='n')]\n",
      "\n",
      "Training docs: 178\n",
      "Testing docs: 60\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "training_set = load_training_data()\n",
    "training_set['real_label'] = training_set['real_label'].replace('f', 'n')\n",
    "\n",
    "documents = []\n",
    "current_doc = []\n",
    "prev = -1\n",
    "for i,word in training_set.iterrows():\n",
    "    if i != prev:\n",
    "        if current_doc:\n",
    "            documents.append(current_doc)\n",
    "        current_doc = []\n",
    "    current_doc.append(row_to_tokenfeatures(word))\n",
    "    prev = i\n",
    "\n",
    "if current_doc:\n",
    "    documents.append(current_doc)\n",
    "\n",
    "print(documents[0])\n",
    "\n",
    "train_docs, test_docs = train_test_split(documents)\n",
    "print()\n",
    "print(f'Training docs: {len(train_docs)}')\n",
    "print(f'Testing docs: {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train = [get_labels(s) for s in train_docs]\n",
    "X_train = [get_features(s) for s in train_docs]\n",
    "\n",
    "y_test = [get_labels(s) for s in test_docs]\n",
    "X_test = [get_features(s) for s in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sizes\n",
    "for features, labels in zip(y_test, X_test):\n",
    "    assert len(features) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_tagger = pycrfsuite.Tagger()\n",
    "crf_tagger.open('model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40 \u00a1 CDMX a Europa en Semana Santa $ 14,984 ! ( Par\u00eds + Ibiza + Venecia )\n",
      "P:   n o    s d      d  d      d     n p      n n n     n n     n n       n \n",
      "C:   n o    s d      n  n      n     n p      n n n     n n     n n       n \n",
      "\n",
      "\n",
      "Incorrectly predicted: 1 out of 60\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "incorrect = 0\n",
    "for i in range(len(test_docs)):\n",
    "    example_sent = documents[i]\n",
    "    predicted = crf_tagger.tag(get_features(example_sent))\n",
    "    correct = get_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = get_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "        \n",
    "        print('P:   ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('C:   ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(test_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40 \u00a1 CDMX a Europa en Semana Santa $ 14,984 ! ( Par\u00eds + Ibiza + Venecia )\n",
      "  P: n o    s d      d  d      d     n p      n n n     n n     n n       n \n",
      "  C: n o    s d      n  n      n     n p      n n n     n n     n n       n \n",
      "\n",
      "\n",
      "  60 \u00a1 CDMX a Noruega $ 10,061 ! ( Y agrega 9 noches de hotel por $ 7,890 !\n",
      "  P: n o    s d       n p      n n n n      n n      n  n     n   n p     n \n",
      "  C: n o    s d       n p      n n n n      n n      n  n     n   n n     n \n",
      "\n",
      "\n",
      " 171 \u00a1 CUN a King \u2019 s Landing ( Croacia ) + \u00c1msterdam $ 12,549 ! Sin escala EE.UU\n",
      "  P: n o   s d    n n n       n n       n n n         n p      n n   n      n     \n",
      "  C: n o   s d    d d d       d d       d d d         n p      n n   n      n     \n",
      "\n",
      "\n",
      " 202 \u00a1 CUN a Washington D . C . $ 3,196 ! Directos ( Y por $ 3,027 adicionales agrega 3 noches de hotel )\n",
      "  P: n o   s d          d d d n n p     n n        n n n   n n     n           n      n n      n  n     n \n",
      "  C: n o   s d          d d d d n p     n n        n n n   n n     n           n      n n      n  n     n \n",
      "\n",
      "\n",
      " 210 Agotados : \u00a1 CDMX a Europa Navidad y A\u00f1o Nuevo $ 16,424 ! ( Madrid + Par\u00eds + Venecia + Berl\u00edn )\n",
      "  P: n        n n o    s d      d       n n   n     n p      n n n      n n     n n       n n      n \n",
      "  C: n        n n o    s d      n       n n   n     n p      n n n      n n     n n       n n      n \n",
      "\n",
      "\n",
      " 401 \u00a1 CUN a Bogot\u00e1 , Colombia \u2013 $ 3,860 ! Opci\u00f3n de hotel 5 d\u00edas y 4 noches por $ 858 p / persona con desayunos ( hab doble ) . Hostal con desayunos por $ 634\n",
      "  P: n o   s d      d d        n n p     n n      n  n     n n    n n n      n   n n   n n n       n   n         n n   n     n n n      n   n         n   n p   \n",
      "  C: n o   s d      d d        n n p     n n      n  n     n n    n n n      n   n n   n n n       n   n         n n   n     n n n      n   n         n   n n   \n",
      "\n",
      "\n",
      " 727 \u00a1 Vuelos + Hotel ! Silao y Puebla a Playa del Carmen \u2013 $ 1,557 . CDMX desde $ 2,451 con desayunos\n",
      "  P: n n      n n     n o     o o      s d     d   d      n n n     n n    n     n n     n   n         \n",
      "  C: n n      n n     n o     o o      s d     d   d      n n p     n n    n     n n     n   n         \n",
      "\n",
      "\n",
      "1310 Vuelo + Hotel . Desde todo M\u00e9xico . CDMX y 23 ciudades m\u00e1s a Santo Domingo , Rep\u00fablica Dominicana \u2013 $ 9,171\n",
      "  P: n     n n     n o     o    o      o o    o o  o        o   s d     d       d d         d          n n p     \n",
      "  C: n     n n     n n     o    o      o o    o o  o        o   s d     d       d d         d          n n p     \n",
      "\n",
      "\n",
      "2125 \u00a1 \u00daltimo minuto ! Oportunidades para semana santa : Cuba + Panam\u00e1 y Chicago\n",
      "  P: n n      n      n n             n    n      n     n n    n n      n n       \n",
      "  C: n n      n      n n             n    n      n     n d    d d      d d       \n",
      "\n",
      "\n",
      "2414 CDMX a Livingstone , Zambia \u2013 $ 21,056 . \u00a1 Con 13 noches de hospedaje , desayunos , vuelos , shuttle aeropuerto-hostal y a las Cataratas Victoria por $ 23,180 MXN !\n",
      "  P: o    s d           d d      n n p      n n n   n  n      n  n         n n         n n      n o       o                 o s d   d         d        n   n n      n   n \n",
      "  C: o    s d           d d      n n p      n n n   n  n      n  n         n n         n n      n n       n                 n n n   n         n        n   n n      n   n \n",
      "\n",
      "\n",
      "Incorrectly predicted: 10 out of 238\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "incorrect = 0\n",
    "for i in range(len(documents)):\n",
    "    example_sent = documents[i]\n",
    "    features = get_features(example_sent)\n",
    "    predicted = crf_tagger.tag(features)\n",
    "    correct = get_labels(example_sent)\n",
    "    if predicted != correct:\n",
    "        incorrect += 1\n",
    "        tokens = get_tokens(example_sent)\n",
    "        lengths = [len(t) for t in tokens]\n",
    "        print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "        \n",
    "        print('  P: ', end='')\n",
    "        for i, token in enumerate(predicted):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print()\n",
    "        print('  C: ', end='')\n",
    "        for i, token in enumerate(correct):\n",
    "            print(token + ( \" \" * lengths[i]), end='')\n",
    "        print('\\n\\n')\n",
    "        \n",
    "print(f'Incorrectly predicted: {incorrect} out of {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 \u00a1 CDMX a Bogot\u00e1 $ 3,467 ! Directos ( Agrega 2 noches de hotel por $ 419 )\n",
      "  P: n o    s d      n p     n n        n n      n n      n  n     n   n n   n \n",
      "\n",
      "\n",
      "   0 \u00a1 CUN a Israel $ 14,574 ! Sin escala EE.UU ( y desde CDMX $ 15,146 )\n",
      "  P: n o   s d      n p      n n   n      n     n n n     n    n n      n \n",
      "\n",
      "\n",
      "   0 \u00a1 CDMX , GDL , VER , MTY , CUN , Silao y TIJ a Lima , Per\u00fa \u2013 $ 6,529 !\n",
      "  P: n o    o o   o o   o o   o o   o o     o o   s d    d d    n n p     n \n",
      "\n",
      "\n",
      "   0 \u00a1 CDMX a Noruega $ 11,863 ! Temporada de Auroras\n",
      "  P: n o    s d       n p      n n         n  n       \n",
      "\n",
      "\n",
      "   0 \u00a1 Tijuana a China + Corea $ 17,522 ! Sin escala EE.UU ( Sem . Santa )\n",
      "  P: n o       s d     d d     n p      n n   n      n     n n   n n     n \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "manual_examples = ['\u00a1CDMX a Bogot\u00e1 $3,467! Directos (Agrega 2 noches de hotel por $419)',\n",
    "                   '\u00a1CUN a Israel $14,574! Sin escala EE.UU (y desde CDMX $15,146)',\n",
    "                   '\u00a1CDMX, GDL, VER, MTY, CUN, Silao y TIJ a Lima, Per\u00fa \u2013 $6,529! ',\n",
    "                   '\u00a1CDMX a Noruega $11,863! Temporada de Auroras',\n",
    "                   '\u00a1Tijuana a China + Corea $17,522! Sin escala EE.UU (Sem. Santa)']\n",
    "documents_as_token_features = [processor.process(example, i) for i, example in enumerate(manual_examples)]\n",
    "documents_as_tagger_features = [get_features(doc) for doc in documents_as_token_features]\n",
    "\n",
    "for i in range(len(documents_as_tagger_features)):\n",
    "    token_features = documents_as_token_features[i]\n",
    "    tagger_features = documents_as_tagger_features[i]\n",
    "    predicted = crf_tagger.tag(tagger_features)\n",
    "    correct = get_labels(token_features)\n",
    "    tokens = get_tokens(token_features)\n",
    "    lengths = [len(t) for t in tokens]\n",
    "    \n",
    "    print(\"%4d\" %  example_sent[0].sentence_id, ' '.join(tokens))\n",
    "\n",
    "    print('  P: ', end='')\n",
    "    for i, token in enumerate(predicted):\n",
    "        print(token + ( \" \" * lengths[i]), end='')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_flat = [item for sublist in y_test for item in sublist]\n",
    "y_pred_flat = []\n",
    "\n",
    "\n",
    "from m16_mlutils.datatools.evaluation import eval_summary\n",
    "\n",
    "for doc in X_test:\n",
    "    predicted = crf_tagger.tag(doc)\n",
    "    y_pred_flat.extend(predicted)\n",
    "\n",
    "metrics, summary, cm = eval_summary(y_test_flat, y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy     0.988267\n",
      "precision    0.982729\n",
      "recall       0.985057\n",
      "f1           0.983786\n",
      "dtype: float64\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           d       0.97      0.93      0.95        91\n",
      "           n       0.99      0.99      0.99       797\n",
      "           o       0.97      1.00      0.99       105\n",
      "           p       1.00      1.00      1.00        57\n",
      "           s       0.98      1.00      0.99        58\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1108\n",
      "   macro avg       0.98      0.99      0.98      1108\n",
      "weighted avg       0.99      0.99      0.99      1108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preserve\n",
    "print(metrics, end='\\n\\n')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
