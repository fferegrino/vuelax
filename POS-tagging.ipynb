{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a tagger for flight offer phrases\n",
    "### Like this one: \"\u00a1CDMX a Bogot\u00e1 \ud83c\udde8\ud83c\uddf4 $4,659!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "HOME = os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "vuelos = pd.read_csv('data/vuelos.csv', index_col=0)\n",
    "with pd.option_context('max_colwidth', 800):\n",
    "    print(vuelos.loc[:100:5][['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the offers follow a simple pattern: *Destination - Origin - Price - Extras*, while extracting this may seem easy for a regular expression, it is not (see this notebook for reference). \n",
    "\n",
    "The idea is to create a tagger that will be able to extract this information, however, one first tag is to identify the information that we want to extract. Following the pattern described above: \n",
    "\n",
    " - **DST**: Destination \n",
    " - **ORI**: Origin \n",
    " - **PRC**: Price \n",
    " - **EXT**: Extras\n",
    " \n",
    "| Text \t| DST \t| ORI \t| PRC \t| OTH \t|\n",
    "|------\t|-----\t|-----\t|-----\t|-----\t|\n",
    "| \u00a1CUN a Holanda \\$8,885! Sin escala EE.UU | CUN | Holanda | 8,885 | Sin escala EE.UU |   \n",
    "| \u00a1CDMX a Noruega <span>$</span>10,061! (Y agrega 9 noches de hotel por \\$7,890!) | CDMX | Noruega | 10,061 | Y agrega 9 noches de hotel por \\$7,890!| \n",
    "| \u00a1Todo M\u00e9xico a Pisa, Toscana Italia \\$12,915! Sin escala EE.UU (Y por \\$3,975 agrega 13 noches hotel) | M\u00e9xico | Pisa, Toscana Italia | 12,915 | Sin escala EE.UU (Y por \\$3,975 agrega 13 noches hotel) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and POS-tag the dataset \n",
    "We need to generate a *csv* file that we can tag (manually \ud83d\ude28) that consists of:\n",
    "```\n",
    "token1    POS tag    Label\n",
    "token2    POS tag    Label\n",
    "token3    POS tag    Label\n",
    "```\n",
    "\n",
    "Where `Label` will be one of DST, ORI, PRC, OTH and NA and will be manually assigned (again: \ud83d\ude28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "spanish_postagger = StanfordPOSTagger(HOME + '/stanford_nlp/models/spanish-distsim.tagger', \n",
    "                                      HOME + '/stanford_nlp/stanford-postagger.jar')\n",
    "\n",
    "print(spanish_postagger.tag('Pepe Pecas pica papas con un pico, con un pico pica papas Pepe Pecas.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknz = TweetTokenizer()\n",
    "\n",
    "transforms = {\n",
    "    'LA': ['Los', 'Angeles']\n",
    "}\n",
    "\n",
    "def index_emoji_tokenize(string, return_flags=False):\n",
    "    i = 0\n",
    "    flag = ''\n",
    "    ix = 0\n",
    "    for t in tknz.tokenize(string):\n",
    "        ix = string.find(t, ix)\n",
    "        if len(t) == 1 and ord(t) >= 127462: # this is the code for \ud83c\udde6\n",
    "            if not return_flags: continue\n",
    "            if flag:\n",
    "                yield flag + t, ix - 1\n",
    "                flag = ''\n",
    "            else:\n",
    "                flag = t\n",
    "        else:\n",
    "            yield t, ix\n",
    "        ix=+1\n",
    "        \n",
    "\n",
    "label = vuelos.iloc[75]['label']\n",
    "print(label)\n",
    "print()\n",
    "tokens = list(index_emoji_tokenize(label, return_flags=True))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simply_tokens = [ l[0] for l in tokens ]\n",
    "print(spanish_postagger.tag(simply_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(label, debug=False):\n",
    "    tokens = list(index_emoji_tokenize(label, True))\n",
    "    if debug:\n",
    "        print('Tokens', len(tokens))\n",
    "    only_tokens = [l[0] for l in tokens]\n",
    "    if debug:\n",
    "        print('Only tokens', len(only_tokens))\n",
    "    positions = [l[1] for l in tokens]\n",
    "    if debug:\n",
    "        print('Positions', len(positions))\n",
    "    tagged = spanish_postagger.tag(only_tokens)\n",
    "    if debug:\n",
    "        print('Tagged', len(tagged))\n",
    "    tags =  [l[1] for l in tagged]\n",
    "    if debug:\n",
    "        print('Tags', len(tags))\n",
    "    lengths =  [len(l) for l in only_tokens]\n",
    "    if debug:\n",
    "        print('Lengths', len(lengths))\n",
    "    n_tokens =  [len(only_tokens) for l in only_tokens]\n",
    "    if debug:\n",
    "        print('N tokens', len(n_tokens))\n",
    "    augmented = ['<p>'] + tags + ['</p>']\n",
    "    uppercase = [all([l.isupper() for l in token]) for token in only_tokens]\n",
    "    return only_tokens, positions, tags, augmented[:len(only_tokens)], augmented[2:], lengths, uppercase, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes quite a while\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('data/i__training_data.csv', 'w') as w:\n",
    "    writer = csv.writer(w)\n",
    "    for i, vuelo in tqdm(list(vuelos.iterrows())):\n",
    "        result = process_label(vuelo['label'])\n",
    "        for row in zip(*result):\n",
    "            writer.writerow(( i, len(vuelo['label']) ) + row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('data/i__training_data.csv', header=None,\n",
    "                            names=['sentence_id', 'offer_len', \n",
    "                                   'token', 'loc', 'pos', 'pos_left', 'pos_right', 'token_len', 'all_upper', 'n_tokens'])\n",
    "print(f'Length {len(training_data)}')\n",
    "training_data.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = process_label(vuelos.iloc[3]['label'], debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(vuelos.iloc[3]['label'])\n",
    "values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
